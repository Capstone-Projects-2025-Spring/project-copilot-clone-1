---
sidebar_position: 1
---
# Unit tests
<!-- For each method, one or more test cases.

A test case consists of input parameter values and expected results.

All external classes should be stubbed using mock objects. -->


## Library Explanation: Each student is required to explain the unit testing library they have chosen. This explanation should include the reasons for selecting the library, its key features, and its suitability for the project.
- Backend: Pytest was chosen for the backend because it integrates well with fastapi. It includes a TestClient for creating mock http requests

## Client
## Server
### Create a Prompt
<details open="True">
- Test <b>/suggest</b> endpoint via <b>POST</b>
    - Input: <b>CodeRequest</b>
    - Expected Result: If input is valid, should return an array of JSON objects generated by OpenAI and a status code of 201
</details>

### Read From Database
<details open="True">
- Test <b>/logs</b> endpoint via <b>GET</b>
    - Input: <b>N/A</b>
    - Expected Result: Returns array of JSON objects along with a status code of 200
</details>

### Write to Database
<details open="True">
- Test <b>/logs</b> endpoint via <b>POST</b>
    - Input: <b>Database Object</b>
    - Expected Result: Returns a status code of 201
</details>

## Database

## Coverage Report: 
<!-- Most unit testing software has the ability to export coverage reports in HTML. To add this to Docusaurus statically, you can place the exported HTML report in the static folder of your Docusaurus project. For automation, you can set up a continuous integration (CI) pipeline that runs your tests, generates the coverage report, and moves it to the static folder every time changes are made to the repository. This ensures your coverage report is always up to date. -->




